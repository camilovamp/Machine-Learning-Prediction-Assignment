---
title: "Analysis"
author: "Camilo Arenas"
date: "May 22, 2015"
output: html_document
---

The first thing I did was to download and open myself the files to make a brief analysis. The first thing that I noticed was that there was a lot of NA values, so for sure I have to make some clean during the process, also the firt column was not necesary (just the id of the row). and coloumn two does not have any meaning

Step 1: load the libraries
```{r}
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
```
Step 2: set the seed
```{r}
set.seed(12345)
```

Step 3 : set url variables with the respective url.
the read.csv, do not acept https so I use only http
```{r}
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
```
Step 4 : Read the files for the training and the testing
```{r}
trainingcsv <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testingcsv <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
```
Step 5: Partition set for Training (60%) and Testing (40%)
```{r}
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
training <- trainingcsv[inTrain, ]
testing <- testingcsv[-inTrain, ]
```
Step 6: clean the data
```{r}
myDataNZV <- nearZeroVar(training, saveMetrics=TRUE)
```
Step 7: create another subset without any NZV variables
```{r}
myNZVvars <- names(training) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
                                      "kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_bel.1","skewness_yaw_belt",
                                      "max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
                                      "var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
                                      "stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
                                      "kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
                                      "max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
                                      "kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
                                      "skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
                                      "amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
                                      "skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
                                      "max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
                                      "amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
                                      "avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
                                      "stddev_yaw_forearm", "var_yaw_forearm")

training <- training[!myNZVvars]
```

Step 8: Remove the first column (the ID)
```{r}
training <- training[c(-1)]
```

Step 9 :Cleaning Variables with too many NAs, variables with more than 60% NA, should be ignore (60% is the training so why to have variables with more than 60% NA???)
```{r}
trainingV3 <- training #variable just for the loop
for(i in 1:length(training)) { 
  if( sum( is.na( training[, i] ) ) /nrow(training) >= .6 ) { #if NAs > 60% of  observations
    for(j in 1:length(trainingV3)) {
      if( length( grep(names(training[i]), names(trainingV3)[j]) ) ==1)  { #columns are the same???
        trainingV3 <- trainingV3[ , -j] 
      }   
    } 
  }
}
training <- trainingV3
rm(trainingV3)
```
Step 10: remove from the testing set the variables that are not in the training set
```{r}
clean1 <- colnames(training)
clean2 <- colnames(training[, -58]) #remove classe
testing <- testing[clean2]
for (i in 1:length(testing) ) {
  for(j in 1:length(training)) {
    if( length( grep(names(training[j]), names(testing)[i]) ) ==1)  {
      class(testing[j]) <- class(training[i])
    }      
  }      
}
```
Step 11: remove column 1, 2 and the one we want to predict (58)
```{r}
testing <- rbind(training[2, -58] , testing)
testing <- testing[-1,]
```

Step 12: creeate one model with rpart and with ramdom forest and make the confution matrix with both of them and the testing, the better one was predictions_2
```{r}
model1 <- rpart(classe ~ ., data=training, method="class")
predictions1 <- predict(model1, testing, type = "class")
confusionMatrix(predictions1, testing$classe)
model2 <- randomForest(classe ~. , data=training)
predictions_1 <- predict(model2, testing, type = "class")
confusionMatrix(predictionsB1, testing$classe)
predictions_2 <- predict(model2, testing, type = "class")
```
Step 13: The code for the submistion of the 20 files. Was correct in the 20 samples :-)
```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predictions_2)
```


